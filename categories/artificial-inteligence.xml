<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Blog Bitson. Aprendimos de unos. Enseñamos a otros. Apostamos a la cultura colectiva. (Publicaciones sobre artificial inteligence)</title><link>http://blog.bitson.group/</link><description></description><atom:link href="http://blog.bitson.group/categories/artificial-inteligence.xml" rel="self" type="application/rss+xml"></atom:link><language>es</language><copyright>Contents © 2018 &lt;a href="mailto:info@bitson.group"&gt;Cooperativa de Trabajo BITSON Ltda.&lt;/a&gt; </copyright><lastBuildDate>Mon, 25 Jun 2018 04:14:04 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Preprocesando datos para Machine Learning</title><link>http://blog.bitson.group/posts/preprocesando-datos-para-machine-learning/</link><dc:creator>nespino</dc:creator><description>&lt;div&gt;&lt;p&gt;A todos nos gustaría llegar a casa y que la cena esté lista. A los algoritmos de &lt;a href="https://es.wikipedia.org/wiki/Aprendizaje_autom%C3%A1tico" title="¿Qué es Machine Learning?"&gt;Machine Learning&lt;/a&gt; también, por eso es que hoy vamos a dejarles cuatro recetas para que puedan alimentar en forma eficiente a sus procesos de entrenamiento.&lt;/p&gt;
&lt;p&gt;Las recetas que repasaremos el día de hoy son:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Binarización&lt;/li&gt;
&lt;li&gt;Eliminación del valor medio&lt;/li&gt;
&lt;li&gt;Escalando&lt;/li&gt;
&lt;li&gt;Normalización&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Cada caso puede requerir el uso de uno o más de estos métodos en forma combinada.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Para simplificarnos las tareas matemáticas, vamos a hacer uso de dos paquetes &lt;a href="https://es.wikipedia.org/wiki/Software_libre_y_de_c%C3%B3digo_abierto" title="Diferencias entre Software Libre y Código Abierto"&gt;Open Source&lt;/a&gt; para desarrollos científicos:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.numpy.org/" title="Numpy"&gt;Numpy&lt;/a&gt;, el cual se puede instalar junto a las de herramientas de &lt;a href="https://www.scipy.org/install.html" title="¿Cómo instalar SciPy?"&gt;SciPy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;El paquete de aprendizaje de Machine Learning &lt;a href="http://scikit-learn.org/stable/install.html" title="Scikit Learn"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ambos se pueden instalar fácilmente desde &lt;strong&gt;pip&lt;/strong&gt; con los siguientes comandos:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;pip install scipy&lt;/p&gt;
&lt;p&gt;pip install scikit-learn&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Primero vamos a importar las librerías necesarias y definir una matriz de ejemplo en numpy para usar en cada uno de los métodos que explicaremos a continuación&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;import numpy as np&lt;/p&gt;
&lt;p&gt;from sklearn import preprocessing&lt;/p&gt;
&lt;p&gt;matriz_ejemplo = np.array([[2.3, 2.9, -1.3, 5.2],&lt;/p&gt;
&lt;p&gt;                        
                   [1.3, -3.8, 2.1, 4.3],&lt;/p&gt;
&lt;p&gt;                        
                   [-8.9, 2.4, -1.1, 0],&lt;/p&gt;
&lt;p&gt;                        
                   [4.1, -3.9, 2.5, 2.1]])&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Binarización&lt;/h2&gt;
&lt;p&gt;Este método es comparable al que utilizan las entradas de las &lt;a href="https://es.wikipedia.org/wiki/Puerta_l%C3%B3gica" title="Compuertas Lógicas"&gt;Compuertas Lógicas&lt;/a&gt; con las que están hechas todos nuestros dispositivos electrónicos, en el que, habiendo definido un valor de umbral, podemos separar a los datos que ingresan de manera que los menores a ese umbral correspondan a 0 (False) y los mayores correspondan a 1 (True). Vamos a transformar esos valores numéricos en valores booleanos, usando un umbral de 2.4 con la función Binarizer:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;matriz_binarizada = preprocessing.Binarizer(threshold=2.4).transform(matriz_ejemplo)&lt;/p&gt;
&lt;p&gt;print("Matriz binarizada:\n", matriz_binarizada)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;y el resultado será:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Matriz binarizada:&lt;/p&gt;
&lt;p&gt;  [[0. 1. 0. 1.]&lt;/p&gt;
&lt;p&gt;  [0. 0. 0. 1.]&lt;/p&gt;
&lt;p&gt;  [0. 0. 0. 0.]&lt;/p&gt;
&lt;p&gt;  [1. 0. 1. 0.]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Observen que el segundo elemento de la tercer fila de la matriz de ejemplo es 2.4, y al binarizarlo con el umbral 2.4 se obtiene un 0 en la misma posición.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pueden encontrar el código completo de este método en el archivo de ejemplo &lt;a href="http://blog.bitson.group/code/ai/binarizacion.py" download&gt;binarizacion.py&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Eliminación del valor medio&lt;/h2&gt;
&lt;p&gt;Este método es muy usado en preprocesamiento de datos para Machine Learning, con el cual se elimina el sesgo de las características en nuestro vector característico. La función que nos permite esta transformación se llama &lt;strong&gt;scale&lt;/strong&gt; y es muy sencilla de usar. Para verificar los resultados paso a paso incluímos algunos &lt;strong&gt;prints&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;print("Matriz de ejemplo:")&lt;/p&gt;
&lt;p&gt;print(matriz_ejemplo)&lt;/p&gt;
&lt;p&gt;print("Promedio =", matriz_ejemplo.mean(axis=0))&lt;/p&gt;
&lt;p&gt;print("Desviación típica =", matriz_ejemplo.std(axis=0))&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;promedio_eliminado = preprocessing.scale(matriz_ejemplo)&lt;/p&gt;
&lt;p&gt;print("\nMatriz procesada:")&lt;/p&gt;
&lt;p&gt;print(promedio_eliminado)&lt;/p&gt;
&lt;p&gt;print("Promedio =", promedio_eliminado.mean(axis=0))&lt;/p&gt;
&lt;p&gt;print("Desviación Típica =", promedio_eliminado.std(axis=0))&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Lo que debería darnos como resultado:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Matriz de ejemplo:&lt;/p&gt;
&lt;p&gt; [[ 2.1  2.9 -1.3  5.2]&lt;/p&gt;
&lt;p&gt; [ 1.3 -3.8  2.1  4.3]&lt;/p&gt;
&lt;p&gt; [-8.9  2.4 -1.1  0. ]&lt;/p&gt;
&lt;p&gt; [ 4.1 -3.9  2.5  2.1]]&lt;/p&gt;
&lt;p&gt;Promedio = [-0.35 -0.6   0.55  2.9 ]&lt;/p&gt;
&lt;p&gt;Desviación típica = [5.04058528 3.25499616 1.75712834 2.01866292]&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Matriz procesada:&lt;/p&gt;
&lt;p&gt; [[ 0.48605467  1.07527009 -1.05285423  1.13936803]&lt;/p&gt;
&lt;p&gt; [ 0.32734294 -0.98310408  0.88212111  0.69352837]&lt;/p&gt;
&lt;p&gt; [-1.69623159  0.92166007 -0.93903215 -1.43659447]&lt;/p&gt;
&lt;p&gt; [ 0.88283399 -1.01382608  1.10976527 -0.39630192]]&lt;/p&gt;
&lt;p&gt;Promedio = [ 0.00000000e+00  0.00000000e+00 -1.11022302e-16  4.16333634e-17]&lt;/p&gt;
&lt;p&gt;Desviación Típica = [1. 1. 1. 1.]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Como pueden observar el promedio en la matriz procesada está cerca de 0 y la desviación típica es 1.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Pueden encontrar el código completo de este método en el archivo de ejemplo &lt;a href="http://blog.bitson.group/code/ai/eliminar_valor_medio.py" download&gt;eliminar_valor_medio.py&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Escalando&lt;/h2&gt;
&lt;p&gt;Es posible que nuestro vector característico incluya valores extremadamente altos y/o extremadamente bajos, por eso es importante escalar esos valores para que nuestro algoritmo opere con valores cercanos, y así evitamos características con valores desorbitantes.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;datos_escalados = preprocessing.MinMaxScaler(feature_range=(0, 1))&lt;/p&gt;
&lt;p&gt;datos_escalados = datos_escalados.fit_transform(matriz_ejemplo)&lt;/p&gt;
&lt;p&gt;print("Datos escalados:\n", datos_escalados)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Podemos comprobar que cada fila fue escalada para que sus valores mínimos y máximos sean 0 y 1 respectivamente, y los demás se transformaron en forma proporcional:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Datos escalados:&lt;/p&gt;
&lt;p&gt; [[0.84615385 1.                 0.
                1.                  ]&lt;/p&gt;
&lt;p&gt;  [0.78461538  0.01470588 0.89473684 0.82692308 ]&lt;/p&gt;
&lt;p&gt;  [0.                  0.92647059 0.05263158 0.
                ]&lt;/p&gt;
&lt;p&gt;  [1.                  0.
                1.
                0.40384615 ]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Pueden encontrar el código completo de este método en el archivo de ejemplo &lt;a href="http://blog.bitson.group/code/ai/escalando.py" download&gt;escalando.py&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Normalización&lt;/h2&gt;
&lt;p&gt;La normalización se usa para modificar los valores en el vector característico y así poder medirlos en una escala común. Existen varios métodos para esto, así que vamos a presentarles los dos más comunes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Normalización L1&lt;/strong&gt;: Busca las menores desviaciones absolutas, y funciona asegurándose que la suma de los valores absolutos en cada fila sea 1.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Normalización L2&lt;/strong&gt;: Busca los menores cuadrados, y funciona asegurándose que la suma de los cuadrados sea 1.&lt;/p&gt;
&lt;p&gt;En general el método de Normalización L1 se considera más robusto que el de Normalización L2, porque el primero es más resistente a valores atípicos en los datos de origen. Es frecuente encontrar valores atípicos y no hay nada que podamos hacer al respecto, por eso buscamos técnicas que los ignoren de forma segura y efectiva durante los cálculos. En cambio, si los valores atípicos son importantes para el problema que estamos resolviendo, podríamos inclinarnos a usar la Normalización L2.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;datos_normalizados_l1 = preprocessing.normalize(matriz_ejemplo, norm='l1')&lt;/p&gt;
&lt;p&gt;datos_normalizados_l2 = preprocessing.normalize(matriz_ejemplo, norm='l2')&lt;/p&gt;
&lt;p&gt;print("Datos con Normalización L1:\n", datos_normalizados_l1)&lt;/p&gt;
&lt;p&gt;print("\nDatos con Normalización L2:\n", datos_normalizados_l2)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Usamos la función &lt;strong&gt;normalize&lt;/strong&gt;, indicando el tipo de normalización, y así podremos apreciar la diferencia por pantalla:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Datos con Normalización L1:&lt;/p&gt;
&lt;p&gt;[[ 0.1826087   0.25217391 -0.11304348  0.45217391]&lt;/p&gt;
&lt;p&gt;[ 0.11304348 -0.33043478  0.1826087   0.37391304]&lt;/p&gt;
&lt;p&gt;[-0.71774194  0.19354839 -0.08870968  0.        ]&lt;/p&gt;
&lt;p&gt;[ 0.32539683 -0.30952381  0.1984127   0.16666667]]&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Datos con Normalización L2:&lt;/p&gt;
&lt;p&gt;[[ 0.32578702  0.44989636 -0.20167768  0.80671072]&lt;/p&gt;
&lt;p&gt;[ 0.20808658 -0.60825309  0.33613986  0.68828639]&lt;/p&gt;
&lt;p&gt;[-0.95870891  0.25852824 -0.11849211  0.        ]&lt;/p&gt;
&lt;p&gt;[ 0.62758369 -0.59696986  0.38267298  0.32144531]]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;em&gt;Pueden encontrar el código completo de este método en el archivo de ejemplo &lt;a href="http://blog.bitson.group/code/ai/normalizacion.py" download&gt;normalizacion.py&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Espero que con estos métodos le den un merecido banquete a sus algoritmos, y que tengan ustedes una deliciosa y nutritiva cena.&lt;/p&gt;&lt;/div&gt;</description><category>ai</category><category>artificial inteligence</category><category>ia</category><category>inteligencia artificial</category><category>machine learning</category><category>numpy</category><category>preprocessing</category><category>python</category><category>scikit</category><category>scipy</category><guid>http://blog.bitson.group/posts/preprocesando-datos-para-machine-learning/</guid><pubDate>Mon, 11 Jun 2018 05:30:15 GMT</pubDate></item></channel></rss>